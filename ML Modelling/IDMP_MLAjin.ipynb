{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d116f99c-43f0-4d04-9b89-ce7b7a44a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2198d49f-483e-4730-b3dc-77d52001c4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration details for connecting to the PostgreSQL database\n",
    "db_config = {\n",
    "    \"dbname\": \"food\",       # Name of the database\n",
    "    \"user\": \"postgres\",     # Username for the database\n",
    "    \"password\": \"postgres\", # Password for the database\n",
    "    \"host\": \"localhost\",    # Host where the database is running\n",
    "    \"port\": \"5432\"          # Port number for the database\n",
    "}\n",
    "\n",
    "# SQL queries to select all data from various tables in the database\n",
    "branded_food = \"SELECT * FROM branded_food\"\n",
    "food = \"SELECT * FROM food\"\n",
    "food_attribute = \"SELECT * FROM food_attribute\"\n",
    "food_attribute_type = \"SELECT * FROM food_attribute_type\"\n",
    "food_nutrient = \"SELECT * FROM food_nutrient\"\n",
    "food_update_log_entry = \"SELECT * FROM food_update_log_entry\"\n",
    "measure_unit = \"SELECT * FROM measure_unit\"\n",
    "microbe = \"SELECT * FROM microbe\"\n",
    "nutrient = \"SELECT * FROM nutrient\"\n",
    "nutrient_incoming_name = \"SELECT * FROM nutrient_incoming_name\"\n",
    "\n",
    "try:\n",
    "    # Establishing a connection to the PostgreSQL database using psycopg2\n",
    "    conn = psycopg2.connect(**db_config)\n",
    "\n",
    "    # Executing SQL queries and loading the results into pandas DataFrames\n",
    "    branded_food = pd.read_sql_query(branded_food, conn)               # Fetch all rows from 'branded_food' table\n",
    "    food = pd.read_sql_query(food, conn)                               # Fetch all rows from 'food' table\n",
    "    food_attribute = pd.read_sql_query(food_attribute, conn)           # Fetch all rows from 'food_attribute' table\n",
    "    food_attribute_type = pd.read_sql_query(food_attribute_type, conn) # Fetch all rows from 'food_attribute_type' table\n",
    "    food_nutrient = pd.read_sql_query(food_nutrient, conn)             # Fetch all rows from 'food_nutrient' table\n",
    "    food_update_log_entry = pd.read_sql_query(food_update_log_entry, conn) # Fetch all rows from 'food_update_log_entry' table\n",
    "    measure_unit = pd.read_sql_query(measure_unit, conn)               # Fetch all rows from 'measure_unit' table\n",
    "    microbe = pd.read_sql_query(microbe, conn)                         # Fetch all rows from 'microbe' table\n",
    "    nutrient = pd.read_sql_query(nutrient, conn)                       # Fetch all rows from 'nutrient' table\n",
    "    nutrient_incoming_name = pd.read_sql_query(nutrient_incoming_name, conn) # Fetch all rows from 'nutrient_incoming_name' table\n",
    "\n",
    "    # Closing the database connection\n",
    "    conn.close()\n",
    "\n",
    "except Exception as e:\n",
    "    # Catching and printing any errors that occur during database connection or query execution\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb0cc07b-1882-48a4-ba72-73cb97b2ccff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merging the 'food' and 'branded_food' tables on the 'fdc_id' column to combine relevant data\n",
    "# Selecting specific columns: 'fdc_id', 'description', 'ingredients', and 'branded_food_category'\n",
    "data_food = food.merge(branded_food, on='fdc_id')[['fdc_id', 'description', 'ingredients', 'branded_food_category']]\n",
    "\n",
    "# Merging the 'food_nutrient' and 'nutrient' tables to combine nutrient-related data\n",
    "# Joining on 'nutrient_id' from 'food_nutrient' and 'id' from 'nutrient'\n",
    "# Selecting specific columns: 'fdc_id', 'nutrient_id', and 'name' (nutrient name)\n",
    "food_nutrients_combined = food_nutrient.merge(nutrient, left_on='nutrient_id', right_on='id')[['fdc_id', 'nutrient_id', 'name']]\n",
    "\n",
    "# Merging the combined nutrient data with the combined food data on 'fdc_id'\n",
    "# This creates a dataset that includes food details along with their nutrient information\n",
    "data = food_nutrients_combined.merge(data_food, on='fdc_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c5291a2-8067-4996-a638-19085f3e0d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, data_count):\n",
    "    # Limit the data to the first 'data_count' rows\n",
    "    data = data[:data_count]\n",
    "\n",
    "    # Combine nutrient names for each 'fdc_id' by grouping and joining them into a single string\n",
    "    data['nutrients'] = data.groupby('fdc_id')['name'].transform(lambda x: ' '.join(x)) \n",
    "\n",
    "    # Remove duplicate nutrient names within each 'fdc_id' by splitting, finding unique values, and rejoining as a string\n",
    "    data['nutrients'] = data['nutrients'].transform(lambda x: np.unique(x.split(' '))) \n",
    "\n",
    "    # Clean up the 'nutrients' column by removing unwanted characters like commas, parentheses, and plus signs\n",
    "    data['nutrients'] = data['nutrients'].transform(lambda x: (\" \".join(x)).replace(\",\", \" \").replace(\"(\", \" \").replace(\")\", \" \").replace(\"+\", \" \"))    \n",
    "\n",
    "    # Drop the columns 'nutrient_id' and 'name' as they are no longer needed\n",
    "    data = data.drop(['nutrient_id', 'name'], axis=1)\n",
    "\n",
    "    # Remove duplicate rows from the dataset\n",
    "    data = data.drop_duplicates()\n",
    "\n",
    "    # Return the processed dataset\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f01eba0-24dd-4824-a79c-f95a0ea580da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(data):\n",
    "    # Convert the 'description' column to lowercase\n",
    "    data['description'] = data['description'].transform(lambda x: x.lower())\n",
    "    # Convert the 'ingredients' column to lowercase\n",
    "    data['ingredients'] = data['ingredients'].transform(lambda x: x.lower())\n",
    "    # Convert the 'nutrients' column to lowercase\n",
    "    data['nutrients'] = data['nutrients'].transform(lambda x: x.lower())\n",
    "    # Convert the 'branded_food_category' column to lowercase and store it in 'food_category'\n",
    "    data['food_category'] = data['branded_food_category'].transform(lambda x: x.lower())\n",
    "\n",
    "    # Remove non-alphanumeric characters from the 'description' column\n",
    "    data['description'] = data['description'].transform(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "    # Remove non-alphanumeric characters from the 'ingredients' column\n",
    "    data['ingredients'] = data['ingredients'].transform(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "    # Remove non-alphanumeric characters from the 'nutrients' column\n",
    "    data['nutrients'] = data['nutrients'].transform(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "    # Remove non-alphanumeric characters from the 'food_category' column\n",
    "    data['food_category'] = data['food_category'].transform(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "    # Uncomment the line below if the NLTK Punkt tokenizer is not already downloaded\n",
    "    # nltk.download('punkt')\n",
    "\n",
    "    # Create a PorterStemmer instance for stemming words\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Function to tokenize, stem, and rejoin the tokens into a string\n",
    "    def stem_string(text):\n",
    "        # Tokenize the input text (convert to lowercase)\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        # Stem each token using the PorterStemmer\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "        # Join the stemmed tokens into a single string\n",
    "        return ' '.join(stemmed_tokens)\n",
    "        \n",
    "    # Apply stemming to the 'description' column\n",
    "    data['description'] = data['description'].apply(stem_string)\n",
    "    # Apply stemming to the 'ingredients' column\n",
    "    data['ingredients'] = data['ingredients'].apply(stem_string)\n",
    "    # Apply stemming to the 'nutrients' column\n",
    "    data['nutrients'] = data['nutrients'].apply(stem_string)\n",
    "\n",
    "    # Reset the index of the DataFrame\n",
    "    data = data.reset_index()\n",
    "    # Drop the old index column\n",
    "    data = data.drop('index', axis=1)\n",
    "    \n",
    "    # Return the processed DataFrame\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b86fc1e-020f-4d75-a0ac-6c773191b1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(data):\n",
    "    # Initialize a LabelEncoder instance to encode categorical labels\n",
    "    le = LabelEncoder()\n",
    "    # Encode the 'food_category' column into numeric values and store it in a new column 'food_category_encoded'\n",
    "    data['food_category_encoded'] = le.fit_transform(data['food_category'])\n",
    "\n",
    "    # Initialize a TfidfVectorizer with specific parameters:\n",
    "    # - Stop words are removed ('english')\n",
    "    # - Maximum of 100 features are retained\n",
    "    # - Terms must appear in at least 1% (min_df=0.01) and at most 90% (max_df=0.9) of the documents\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=100, min_df=0.01, max_df=0.9)\n",
    "\n",
    "    # Apply TF-IDF vectorization to the 'description' column\n",
    "    tfidf_matrix_description = vectorizer.fit_transform(data['description'])  \n",
    "    # Convert the TF-IDF matrix for 'description' into a DataFrame\n",
    "    tfidf_df_description = pd.DataFrame(\n",
    "        tfidf_matrix_description.toarray(), \n",
    "        columns=vectorizer.get_feature_names_out()\n",
    "    )\n",
    "    \n",
    "    # Apply TF-IDF vectorization to the 'ingredients' column\n",
    "    tfidf_matrix_ingredients = vectorizer.fit_transform(data['ingredients'])  \n",
    "    # Convert the TF-IDF matrix for 'ingredients' into a DataFrame\n",
    "    tfidf_df_ingredients = pd.DataFrame(\n",
    "        tfidf_matrix_ingredients.toarray(), \n",
    "        columns=vectorizer.get_feature_names_out()\n",
    "    )\n",
    "    \n",
    "    # Apply TF-IDF vectorization to the 'nutrients' column\n",
    "    tfidf_matrix_nutrients = vectorizer.fit_transform(data['nutrients'])  \n",
    "    # Convert the TF-IDF matrix for 'nutrients' into a DataFrame\n",
    "    tfidf_df_nutrients = pd.DataFrame(\n",
    "        tfidf_matrix_nutrients.toarray(), \n",
    "        columns=vectorizer.get_feature_names_out()\n",
    "    )\n",
    "\n",
    "    # Initialize an empty DataFrame to store the combined features\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Concatenate the TF-IDF DataFrames for 'description', 'ingredients', and 'nutrients' along columns\n",
    "    df = pd.concat([tfidf_df_description, tfidf_df_ingredients, tfidf_df_nutrients], axis=1)\n",
    "    \n",
    "    # Add the encoded food category as the target variable to the DataFrame\n",
    "    df['category'] = data['food_category_encoded']\n",
    "    \n",
    "    # Return the feature-engineered DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e25104c0-bd83-4a07-8e96-ffcb76636bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_into_db(predictions, data, accuracy):\n",
    "    # Establish a connection to the database using the provided configuration\n",
    "    connection = psycopg2.connect(**db_config)\n",
    "\n",
    "    # Create a cursor object to execute SQL queries\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    # Iterate through the actual and predicted values from the predictions DataFrame\n",
    "    for i, j in zip(predictions['y_test'], predictions['y_pred']):\n",
    "        # SQL query to insert data into the 'predictions' table\n",
    "        insert_query = \"\"\"\n",
    "                        INSERT INTO predictions (model_name, accuracy, actual_value, predicted_value)\n",
    "                        VALUES (%s, %s, %s, %s);\n",
    "                        \"\"\"\n",
    "        # Execute the SQL query with parameters:\n",
    "        # - Model name: \"RandomForestClassifier\"\n",
    "        # - Model accuracy: provided accuracy value\n",
    "        # - Actual value: Retrieve the actual category name based on 'y_test'\n",
    "        # - Predicted value: Retrieve the predicted category name based on 'y_pred'\n",
    "        cursor.execute(insert_query, (\"RandomForestClassifier\", accuracy, \n",
    "                                      data[data['food_category_encoded'] == i]['food_category'].values[0], \n",
    "                                      data[data['food_category_encoded'] == j]['food_category'].values[0]))\n",
    "\n",
    "    # Commit the transaction to save changes to the database\n",
    "    connection.commit()\n",
    "    # Close the cursor to release database resources\n",
    "    cursor.close()\n",
    "    # Close the database connection\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "510ce4cb-f3e3-40e3-bd37-9b2967c7bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model_metrics(df, data):\n",
    "    # Separate the features (X) and target variable (y) from the dataset\n",
    "    X = df.drop('category', axis=1)  # Features excluding the target variable\n",
    "    y = df['category']  # Target variable\n",
    "\n",
    "    # Split the data into training and testing sets (80% training, 20% testing)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
    "\n",
    "    # Apply PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=0.95)  # Retain 95% of the variance\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "\n",
    "    # Initialize the Random Forest Classifier with a fixed random seed for reproducibility\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    \n",
    "    # Train the model using the training data\n",
    "    model.fit(X_train_pca, y_train)\n",
    "\n",
    "    # Use the trained model to make predictions on the test data\n",
    "    y_pred = model.predict(X_test_pca)\n",
    "\n",
    "    # Calculate evaluation metrics for the model\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),  # Accuracy of predictions\n",
    "        'precision': precision_score(y_test, y_pred, average='weighted'),  # Macro-averaged precision\n",
    "        'f1_score': f1_score(y_test, y_pred, average='weighted'),  # Weighted F1 score\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame to store the actual and predicted values\n",
    "    predictions = {\"y_test\": y_test, \"y_pred\": y_pred}\n",
    "    predictions = pd.DataFrame(predictions)\n",
    "    \n",
    "    # Insert the predictions and metrics into the database\n",
    "    insert_into_db(predictions, data, metrics['accuracy'])\n",
    "    \n",
    "    # Return the calculated metrics\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3bb3624-dc44-43e8-aef2-fba31bd023e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_into_metrics(metrics, size):\n",
    "    # Establish a connection to the database using the configuration\n",
    "    connection = psycopg2.connect(**db_config)\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    # Prepare a list of metrics with their names and rounded values\n",
    "    metrics_final = [\n",
    "        {\"metric_name\": \"Accuracy\", \"metric_value\": round(metrics['accuracy'], 2)},  # Round accuracy to 2 decimal places\n",
    "        {\"metric_name\": \"Precision\", \"metric_value\": round(metrics['precision'], 2)},  # Round precision to 2 decimal places\n",
    "        {\"metric_name\": \"F1 Score\", \"metric_value\": round(metrics['f1_score'], 2)},  # Round F1 score to 2 decimal places\n",
    "    ]\n",
    "    \n",
    "    # Insert each metric into the `performance_metrics` table\n",
    "    for metric in metrics_final:\n",
    "        query = \"\"\"\n",
    "            INSERT INTO performance_metrics (model_name, metric_name, metric_value, data_count)\n",
    "            VALUES (%s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        # Execute the query with the corresponding values\n",
    "        cursor.execute(query, (\"RandomForestClassifier\", metric[\"metric_name\"], metric[\"metric_value\"], size))\n",
    "    \n",
    "    # Commit the transaction to save changes to the database\n",
    "    connection.commit()\n",
    "    \n",
    "    # Close the cursor and database connection\n",
    "    cursor.close()\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5b1d5ab-6ddf-4b56-949a-036337594193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(data, data_count):\n",
    "    # Preprocess the data to filter and clean it up to the given count\n",
    "    data = preprocess_data(data, data_count)\n",
    "    \n",
    "    # Perform text processing on the data to standardize text features\n",
    "    data = text_processing(data)\n",
    "    \n",
    "    # Generate feature representations from the processed data\n",
    "    df = feature_engineering(data)\n",
    "    \n",
    "    # Train the model, evaluate its performance, and insert predictions into the database\n",
    "    metrics = training_model_metrics(df, data)\n",
    "    \n",
    "    # Save the calculated performance metrics into the database\n",
    "    insert_into_metrics(metrics, df.shape[0])\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19faaebb-1ede-4dc3-894d-0f3861734faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Model:  92.31 %\n",
      "Precision of the Model:  85.9 %\n",
      "F1 Score of the Model:  88.81 %\n"
     ]
    }
   ],
   "source": [
    "output = run_model(data, 1000)\n",
    "\n",
    "print(\"Accuracy of the Model: \", round(output['accuracy'] * 100, 2), \"%\")\n",
    "print(\"Precision of the Model: \", round(output['precision'] * 100, 2), \"%\")\n",
    "print(\"F1 Score of the Model: \", round(output['f1_score'] * 100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27612bc1-3e7b-45c3-ac7f-fdad151f991d",
   "metadata": {},
   "source": [
    "### 1. **`preprocess_data(data, data_count)`**:\n",
    "   - **Purpose**: Cleans and prepares the data for analysis. It limits the dataset to the first `data_count` rows, processes nutrients by combining and cleaning them, and removes duplicates and unnecessary columns like `nutrient_id` and `name`.\n",
    "   - **Use**: Prepares the dataset by simplifying nutrient data and ensuring no redundancy, making it suitable for further processing.\n",
    "\n",
    "### 2. **`text_processing(data)`**:\n",
    "   - **Purpose**: Standardizes and preprocesses text data (such as descriptions, ingredients, and nutrients). It converts text to lowercase, removes non-alphanumeric characters, and applies stemming to words to reduce them to their base forms.\n",
    "   - **Use**: Ensures consistency in the text data by removing noise and reducing words to their root form for better model performance.\n",
    "\n",
    "### 3. **`feature_engineering(data)`**:\n",
    "   - **Purpose**: Transforms raw data into meaningful features for modeling. It encodes the food categories using a `LabelEncoder`, and applies TF-IDF vectorization to the text columns (description, ingredients, and nutrients) to convert them into numerical features.\n",
    "   - **Use**: Prepares the data for machine learning by converting categorical and text-based features into a format that can be used by models.\n",
    "\n",
    "### 4. **`insert_into_db(predictions, data, accuracy)`**:\n",
    "   - **Purpose**: Inserts the predicted and actual food categories into a database, along with the model's accuracy. This allows tracking of predictions and their corresponding accuracies.\n",
    "   - **Use**: Saves the model's predictions to a database for future reference or analysis.\n",
    "\n",
    "### 5. **`training_model_metrics(df, data)`**:\n",
    "   - **Purpose**: Splits the data into training and testing sets, trains a `RandomForestClassifier` model on the data, makes predictions, and calculates evaluation metrics like accuracy, precision, and F1 score.\n",
    "   - **Use**: Trains the machine learning model and evaluates its performance using relevant metrics. These metrics help assess the model's effectiveness.\n",
    "\n",
    "### 6. **`insert_into_metrics(metrics, size)`**:\n",
    "   - **Purpose**: Inserts the model's performance metrics (accuracy, precision, F1 score) into a database along with the size of the data used for training.\n",
    "   - **Use**: Tracks and stores the performance of the model for each run in a database, allowing future comparisons or performance tracking.\n",
    "\n",
    "### 7. **`run_model(data, data_count)`**:\n",
    "   - **Purpose**: Orchestrates the entire process of running the model, from preprocessing the data, applying text processing, feature engineering, training the model, and storing the results (predictions and metrics) in the database.\n",
    "   - **Use**: The main entry point to execute the entire machine learning pipeline, automating data preparation, model training, and result storage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
